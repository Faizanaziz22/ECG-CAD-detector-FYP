{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a350f24-bc89-4fee-b624-fece16867c08",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (569129034.py, line 114)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 114\u001b[1;36m\u001b[0m\n\u001b[1;33m    y_pred = np.argmax(model.predict(X_test), axis=1)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# --- Load all signal_*.npy files + labels.npy ---\n",
    "def load_signals_from_folder(folder_path, label_path):\n",
    "    signal_files = sorted(glob(os.path.join(folder_path, \"signal_*.npy\")))\n",
    "    if not signal_files:\n",
    "        raise FileNotFoundError(\"No signal_*.npy files found in folder.\")\n",
    "\n",
    "    print(f\"üìÅ Found {len(signal_files)} signal files.\")\n",
    "    X = np.array([np.load(f) for f in signal_files])\n",
    "    y = np.load(label_path)\n",
    "    return X, y\n",
    "\n",
    "# --- Lightweight augmentation ---\n",
    "def augment_each_class(X, y, target_per_class=500):\n",
    "    X_aug, y_aug = [], []\n",
    "    for label in np.unique(y):\n",
    "        class_signals = X[y == label]\n",
    "        current_count = len(class_signals)\n",
    "        needed = target_per_class - current_count\n",
    "        augmented = []\n",
    "\n",
    "        while len(augmented) < needed:\n",
    "            for sig in class_signals:\n",
    "                aug_sig = sig.copy()\n",
    "                if random.random() < 0.5:\n",
    "                    aug_sig = aug_sig[::-1]\n",
    "                augmented.append(aug_sig)\n",
    "                if len(augmented) >= needed:\n",
    "                    break\n",
    "\n",
    "        all_signals = np.concatenate([class_signals, np.array(augmented)])\n",
    "        labels = np.full(target_per_class, label)\n",
    "        X_aug.append(all_signals)\n",
    "        y_aug.append(labels)\n",
    "\n",
    "    return np.concatenate(X_aug), np.concatenate(y_aug)\n",
    "\n",
    "# --- Simple Dense Model for LDA output ---\n",
    "def build_dense_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Dense(64, input_shape=(input_shape,), activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# --- Main Pipeline ---\n",
    "def main():\n",
    "    folder_path = r\"C:\\Users\\faizan\\ECG_detection\\cleaned_signals\"\n",
    "    label_path = os.path.join(folder_path, \"labels.npy\")\n",
    "\n",
    "    # Load dataset\n",
    "    X, y = load_signals_from_folder(folder_path, label_path)\n",
    "    print(f\"‚úÖ Loaded signals: {X.shape}, labels: {y.shape}\")\n",
    "\n",
    "    # Balance data with augmentation\n",
    "    X, y = augment_each_class(X, y, target_per_class=500)\n",
    "    print(f\"üîÅ After augmentation: {X.shape}\")\n",
    "    print(\"üìä Class distribution:\", Counter(y))\n",
    "\n",
    "    # Normalize signals\n",
    "    X = (X - np.mean(X, axis=1, keepdims=True)) / (np.std(X, axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "    # Apply LDA\n",
    "    lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "    X_lda = lda.fit_transform(X, y)\n",
    "\n",
    "    # Visualize LDA projection\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for label in np.unique(y):\n",
    "        plt.scatter(X_lda[y == label, 0], X_lda[y == label, 1], label=f'Class {label}', alpha=0.6)\n",
    "    plt.legend()\n",
    "    plt.title(\"LDA Projection (2D)\")\n",
    "    plt.xlabel(\"LD1\")\n",
    "    plt.ylabel(\"LD2\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Train/Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_lda, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    y_train_cat = to_categorical(y_train, 3)\n",
    "    y_test_cat = to_categorical(y_test, 3)\n",
    "\n",
    "    # Build and train model\n",
    "    model = build_dense_model(input_shape=X_train.shape[1], num_classes=3)\n",
    "    model.fit(X_train, y_train_cat, epochs=30, batch_size=32, validation_split=0.1, verbose=1)\n",
    "    # ‚úÖ Save the trained model to disk\n",
    "    model_save_path = os.path.join(folder_path, \"CNN+LSTM_dense_model.h5\")\n",
    "    model.save(model_save_path)\n",
    "    print(f\"‚úÖ Model saved to: {model_save_path}\")\n",
    "\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"\\nüßæ Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"‚úÖ Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Class 0', 'Class 1', 'Class 2'],\n",
    "                yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09039b6-71ef-45dd-980c-0109b0da53cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43f9530-24d8-4fec-ad48-78c0e8ba8451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfenv)",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
